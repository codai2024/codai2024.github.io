# CODAI: COuntering Disinformation with Artificial Intelligence
The first edition of CODAI will be co-located with ECAI, in S19-24 OCTOBER 2024 Santiago de Compostela.


Social media platforms which have been designed primarily to allow users to create and share content with others, have become integral parts of modern communication, enabling people to connect with friends, family as well as for broadcasting information to a wider audience. On one side these platforms provide an opportunity to facilitate discussions in an open and free environment.  On the flip side, new societal issues have started emerging on these platforms.  Among all the issues, the topic of misinformation has been prevalent on these platforms. The term misinformation is an umbrella term which encompasses various entities such as fake news, hoaxes, rumors to name a few. While misinformation refers to non-intentional spread of non-authentic information, the term disinformation points to spreading of a piece of inauthentic information with certain malign intentions.  


Initially, researchers have mainly focused on identifying and characterizing misinformation using text based techniques through traditional and advanced NLP techniques. However, with the advancement of techniques and availability of various AI tools, the (mis)information has started appearing in the form of multimodality. For example, a piece of image with incorrect text embedded on it or a morphed video with audio. In addition, the topic of misinformation has impacted individuals and communities from various domains such as medical, political, entertainment, business, etc. This calls for combining forces from different domains. In other words, to counter misinformation computer scientists need to work with domain specialists. To understand the intention a psychologist's inputs can also be vital to understand the reasons for the spreading of misinformation. To summarize, a holistic view is needed to counter the menace of misinformation spread on online social media platforms.

The International Workshop ‚ÄúCODAI: COuntering Disinformation with Artificial Intelligence‚Äù provides a platform for researchers from various domains to come together and not only present their works but also provide an ecosystem to discuss ideas which can facilitate countering the spread of misinformation. 


## Topics of interest
Areas of interest to include, but are not limited to, the following:


### Network level

- Information diffusion models for understanding and thwarting the spread of low-quality information;
-  Understanding and detection of disinformation;
- Characterization and detection of coordinated inauthentic behavior;
- Novel techniques for detecting malicious accounts (e.g., bots, cyborgs and trolls);
- Graph mining and network analysis approaches for studying polarized communities and for reducing polarization;


### Content level

- Information diffusion models for understanding and thwarting the spread of low-quality information;
-  Understanding and detection of disinformation;
- Characterization and detection of coordinated inauthentic behavior;
- Novel techniques for detecting malicious accounts (e.g., bots, cyborgs and trolls);
- Graph mining and network analysis approaches for studying polarized communities and for reducing polarization;


### Context level

- Study, inference and detection of narratives in disinformation campaigns;
- Impact/Harm of misinformation on society.
- Case-studies on the spread and impact of fake news in controversial topics such as politics, health, climate change, economics, migration.
- Social and psychological studies, or data analytics related to misinformation spreaders.

## Evaluation

Metrics, tools and methods for measuring the impact of fake news and of coordinated inauthentic behaviors;
Datasets for evaluation

This workshop will cover works that not only provide studies, characterize or model mis- and dis-information but where state-of-the-art frameworks can also be discussed for countering not only uni but also multi-model. We also invite works related to coordinated inauthentic behavior and information operations in different forms of misinformation, that is rumors, fake news, etc.


## Schedule

`9.00-9.30`   Welcome

`9.30-10.30` Invited talk - fact-checking field

`10.30-11.00` Coffee - networking  ‚òï

`11.00-13:00` Paper presentations

`13:00-14:00` Lunch break ü•™

`14:00-15:00` Invited talk (NLP and misinformation)

`15:00-16.30` Paper presentations

`16:30-17:00` Coffee break - networking

`17:00-18:00` Discussion panel

`18:00-18.30` Final words



<!--
`9:00 -  9:10` Opening remarks

`9:10 - 10:00` Invited talk by **Prof.XXX**

`10:00 - 10:30` 2 oral presentations:
1. Topic
2. Topic 

`10:30 - 11:00` Break

`11:00 - 12:30` In-person & virtual **poster** sessions 

`12:30 - 14:00` Lunch ü•™

`14:00 - 15:30` 6 oral presentations:
1. X
2. X
3. X
4. X
5. X
6. X

`15:30 - 16:00` Break ‚òï

`16:00 - 16:50` Invited talk by **Prof. X**

`16:50 - 17:00` Closing remarks and awards

`17:00 - 18:00` Panel discussion on **Mechanistic Interpretability** with:
- Prof A
- Prof B
- Prof C
- Prof D

--->

## Invited Speakers
### A
<p><img src="img/Zhijing.png" width="150px" height="140px"></p>

_Zhijing Jin is a Ph.D. at Max Planck Institute & ETH. Her research focuses on socially responsible NLP via causal and moral principles. Specifically, she works on expanding the impact of NLP by promoting NLP for social good, and developing CausalNLP to improve robustness, fairness, and interpretability of NLP models, as well as analyze the causes of social problems._

**Causal NLP: A Path towards Opening the Black Box of NLP**<br>
> Recent advancements in large language models (LLMs) .


### B
<p><img src="img/Antoine.jpg" width="150px" height="150px"></p>

A is a Ph.D. at Max Planck Institute & ETH. Her research focuses on socially responsible NLP via causal and moral principles. Specifically, she works on expanding the impact of NLP by promoting NLP for social good, and developing CausalNLP to improve robustness, fairness, and interpretability of NLP models, as well as analyze the causes of social problems._

**From Mechanistic Interpretability to Mechanistic Reasoning**
> Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. Despite this observation, our best methods for interpreting these representations yield few actionable insights on how to manipulate this parameter space for downstream benefit. In this talk, I will present work on methods that simulate ....

## Panel Discussion on "Mechanistic Interpretability"
Panelists: 
- John Hewitt, Stanford
- Zhijing Jin, Max Planck Institute & ETH
- Antoine Bosselut, EPFL
- Neel Nanda, DeepMind

## Important dates

 - Submission deadline: 15th May 2024 
 - Accept/Reject Communications: 1st July 2024
 - Camera-ready papers due: 22nd July 2023
 - Workshop date:  19 October 2024




All deadlines are 11:59pm UTC-12 ("anywhere on earth").

## Workshop description

Many recent performance improvements in NLP have come at the cost of understanding of the systems. How do we assess what representations and computations models learn? How do we formalize desirable properties of interpretable models, and measure the extent to which existing models achieve them? How can we build models that better encode these properties? What can new or existing tools tell us about these systems‚Äô inductive biases?

The goal of this workshop is to bring together researchers focused on interpreting and explaining NLP models by taking inspiration from fields such as machine learning, psychology, linguistics, and neuroscience. We hope the workshop will serve as an interdisciplinary meetup that allows for cross-collaboration.

The topics of the workshop include, but are not limited to:
- Explanation methods such as saliency, attribution, free-text explanations, or explanations with structured properties.
- Mechanistic interpretability, reverse engineering approaches to understanding particular properties of neural models.
- Probing methods for testing whether models have acquired or represent certain linguistic properties.
- Applying analysis techniques from other disciplines (e.g., neuroscience or computer vision).
- Examining model performance on simplified or formal languages.
- Proposing modifications to neural architectures that increase their interpretability.
- Open-source tools for analysis, visualization, or explanation.
- Evaluation of explanation methods: how do we know the explanation is faithful to the model?
- Opinion pieces about the state of explainable NLP.

Feel free to reach out to the organizers at the email below if you are not sure whether a specific topic is well-suited for submission.

## Call for Papers
We will accept submissions through Softconf at: [https://www.softconf.com/emnlp2023/blackboxnlp2023/](https://www.softconf.com/emnlp2023/blackboxnlp2023/). All submissions should use the EMNLP 2023 [template](https://www.overleaf.com/latex/templates/instructions-for-emnlp-2023-proceedings/scyjxmtnrskr) and formatting requirements specified by [ACL](https://acl-org.github.io/ACLPUB/formatting.html). Archival paper must be fully anonymized. Submissions of both types can be made through [Softconf](https://www.softconf.com/emnlp2023/blackboxnlp2023/).


### Submission Types
- **Archival papers** of up to 8 pages + references. These are papers reporting on completed, original and unpublished research. An optional appendix may appear after the references in the same pdf file. Accepted papers are expected to be presented at the workshop and will be published in the workshop proceedings of the ACL Anthology, meaning they cannot be published elsewhere. They should report on obtained results rather than intended work. Broader Impacts/Ethics and Limitations sections are optional and can be included on a 9th page.
- **Non-archival extended abstracts** of 2 pages + references. These may report on work in progress or may be cross-submissions of work that has already appeared (or is scheduled to appear) in another venue in 2022-2023. Abstract titles will be posted on the workshop website but will not be included in the proceedings. The selection will not be based on a double-blind review and thus submissions of this type need not be anonymized. 

Accepted submissions for both tracks will be presented at the workshop: most as posters, some as oral presentations (determined by the program committee).


### Dual Submissions and Preprints
Dual submissions **are** allowed for the archival track, but please check the dual submissions policy for the other venue that you are dual-submitting to. Papers posted to preprint servers such as arXiv can be submitted without any restrictions on when they were posted.

### Camera-ready information
Authors of accepted archival papers should upload the final version of their paper to the submission system by the camera-ready deadline. Authors may use **one extra page** to address reviewer comments, for a total of nine pages + references. Broader Impacts/Ethics and Limitations sections are optional and can be included on a 10th page.

## Contact
Please contact the organizers at <a href="mailto:blackboxnlp@googlegroups.com">blackboxnlp@googlegroups.com</a> for any questions.

## Sponsors

* Ex Google
* EX Apple

## Organizers

You can reach the organizers by e-mail to <a href="mailto:codaihelp@gmail.com">codaihelp@gmail.com</a>.

### Rajesh Sharma
Rajesh Sharma, Head, CSS Lab, University of Tartu, Estonia, Email: rajesh.sharma@ut.ee (Primary contact)
Rajesh Sharma is working as an Associate Professor of Information Systems at the Institute of Computer Science at the University of Tartu, Estonia. His interests include Big data analytics, especially in the domain of Social Media and Social Network Analysis. He has published papers in ICWSM, IEEE Big data and IEEE/ACM ASONAM conferences and in journals such as International Journal of Data Science and Analytics, IEEE Transactions on Network Science and Engineering, Journal of Social Network Analysis and Mining. In particular, he has published several papers on (mis)information diffusion on single and multilayer networks. He has served on the advisory board members of an IMF project about detecting conflicts of interest in public procurement. He is also associate editor for the Elsevier Journal of Social Network Analysis and Mining and is also a PC member for the ASONAM conference. He is also part of TPC of Complex Networks, and SocInfo Conferences. He was an invited speaker at the Digital Humanities Workshop, Estonia. Presently, he is leading efforts on SoBigData++ research infrastructure project, and two CHIST-ERA projects, namely SAI and HAMISON. In the past, he was part of SoBigData (H2020) project and InWeGe (EU commission project related to the gender pay gap in Estonia). He is also involved in an Industrial project with Swedbank, one of the largest banks in Baltics. He was one of the organizers for the ‚ÄúDisInfo‚Äù  workshop (which was related to DisInformation behavior) that happened at the International Conference on Social Informatics (SocInfo) 2020.  https://kdd.isti.cnr.it/socinfo2020/workshops.html

### Anselmo Pe√±as
Anselmo Pe√±as, NLP & IR UNED, Universidad Nacional de Educaci√≥n a Distancia (UNED) Email: anselmo@lsi.uned.es
Anselmo Pe√±as is full professor of computer science at UNED. He holds the Award of the Spanish Society for Natural Language Processing. In 2010 he stayed at the University of Southern California as a visiting scholar. In 2016 he stayed 6 months in the University of York working on unsupervised machine learning techniques applied to natural language interpretation. He has participated in several EU projects and, currently, he is the international coordinator of EU CHIST-ERA HAMISON project (2023-2025) on the Holistic Analysis of Disinformation. From 2007 to 2015, he acted as the international coordinator of the European Question Answering benchmarking and evaluation campaigns in multiple European Languages at the Cross-Language Evaluation Forum (CLEF QA Track).


## Program Committee

**Rodrigo Agerri**, HiTZ Center - Ixa, University of the Basque Country (UPV/EHU)
**Paolo Rosso**, Natural Language Engineering Lab., Universidad Polit√©cnica de Valencia (UPV).
**Arkaitz Zubiaga**,  Social Data Science lab , Queen Mary University of London
**Harith Alani**, Knowledge Media Institute, Open University, London
**Anwitaman Datta**, Nanyang Technological University, Singapore
**Johannes Langguth**, Simula Research Laboratory, Norway
**Serena Villata**, Centre national de la recherche scientifique Universit√† degli Studi di Torino, Italy‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ‚ÄÇ
**Elena Cabrio**, Universit√© C√¥te d'Azur, France
**David Camacho**, Applied Intelligence & Data Analysis group, Universidad Polit√©cnica de Madrid (UPM)
**Anselmo Pe√±as**, NLP & IR UNED, Universidad Nacional de Educaci√≥n a Distancia (UNED)
**Roberto Centeno**, NLP & IR UNED, Universidad Nacional de Educaci√≥n a Distancia (UNED).
**√Ålvaro Rodrigo**, NLP & IR UNED, Universidad Nacional de Educaci√≥n a Distancia (UNED).
**Rajesh Sharma**, CSS Lab, University of Tartu.
**Mudit Dhawan**, Microsoft Research
**Neha Pathak**, Indian Institute of Information Technology (IIIT) Delhi
**Giulio Rossetti**, CNR, Pisa
**Jan Milan**, Applied University of Science, Zurich
**R√©my Cazabet**, Univ. Lyon 1, Lyon, France



## Anti-Harassment Policy
BlackboxNLP 2023 adheres to the [ACL Anti-Harassment Policy](https://www.aclweb.org/adminwiki/sphp?title=Anti-Harassment_Policy).


[sc]: https://scaledcognition.com
